{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8648ef28",
   "metadata": {},
   "source": [
    "# Phase A ? Teacher Baseline & KD Logits\n",
    "\n",
    "Loads a public BERT-large SST-2 checkpoint, evaluates dev/test metrics, and exports KD subsets (1k + 500) with teacher logits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f54bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root() -> Path:\n",
    "    current = Path.cwd().resolve()\n",
    "    for path in [current, *current.parents]:\n",
    "        if (path / \"src\").exists() and (path / \"notebooks\").exists():\n",
    "            return path\n",
    "    raise RuntimeError(\"Unable to locate the repository root. Please run this notebook from inside the project.\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "os.chdir(PROJECT_ROOT)\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d53421",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from src import cost, data, eval as eval_utils, models, utils\n",
    "from src.utils import GLOBAL_CONFIG, configure_tf32, set_seed_everywhere\n",
    "\n",
    "teacher_checkpoint = \"textattack/bert-large-uncased-SST-2\"\n",
    "\n",
    "set_seed_everywhere(GLOBAL_CONFIG.seed)\n",
    "configure_tf32(GLOBAL_CONFIG.tf32)\n",
    "\n",
    "raw_dataset = data.load_sst2()\n",
    "model, tokenizer = models.load_model_and_tokenizer(teacher_checkpoint)\n",
    "tokenized = data.tokenize_text_dataset(raw_dataset, tokenizer, GLOBAL_CONFIG.max_length)\n",
    "\n",
    "validation_dataset = data.format_for_torch(tokenized[\"validation\"])\n",
    "test_dataset = data.format_for_torch(tokenized[\"test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4420caec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = utils.get_device()\n",
    "val_metrics, _ = eval_utils.evaluate_model(\n",
    "    model,\n",
    "    validation_dataset,\n",
    "    GLOBAL_CONFIG.per_device_eval_batch_size,\n",
    "    device=device,\n",
    ")\n",
    "test_metrics, _ = eval_utils.evaluate_model(\n",
    "    model,\n",
    "    test_dataset,\n",
    "    GLOBAL_CONFIG.per_device_eval_batch_size,\n",
    "    device=device,\n",
    ")\n",
    "print(\"Validation\", val_metrics)\n",
    "print(\"Test\", test_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52030deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kd_specs = [(\"kd_1000\", 1000), (\"kd_500\", 500)]\n",
    "kd_output_base = Path(\"outputs\") / \"kd\"\n",
    "kd_output_base.mkdir(parents=True, exist_ok=True)\n",
    "kd_paths = {}\n",
    "\n",
    "for subset_name, size in kd_specs:\n",
    "    subset = data.sample_subset(raw_dataset[\"train\"], sample_size=size, seed=GLOBAL_CONFIG.seed)\n",
    "    subset_tokenized = data.tokenize_text_dataset(subset, tokenizer, GLOBAL_CONFIG.max_length)\n",
    "    subset_for_logits = data.format_for_torch(subset_tokenized)\n",
    "    logits = eval_utils.generate_logits(\n",
    "        model,\n",
    "        subset_for_logits,\n",
    "        GLOBAL_CONFIG.per_device_eval_batch_size,\n",
    "        device=device,\n",
    "    )\n",
    "    subset_with_logits = data.add_teacher_logits(subset, logits.tolist())\n",
    "    target_dir = kd_output_base / subset_name\n",
    "    data.save_dataset(subset_with_logits, target_dir)\n",
    "    kd_paths[subset_name] = str(target_dir)\n",
    "    print(f\"Saved {subset_name} ({size} samples) to {target_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c6d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reports_dir = utils.ensure_dir(\"outputs/reports\")\n",
    "metrics = {\n",
    "    \"phase\": \"A\",\n",
    "    \"checkpoint\": teacher_checkpoint,\n",
    "    \"dev\": val_metrics,\n",
    "    \"test\": test_metrics,\n",
    "    \"parameter_counts\": cost.count_parameters(model),\n",
    "    \"kd_subsets\": kd_paths,\n",
    "}\n",
    "utils.write_json(metrics, reports_dir / \"phase_a_metrics.json\")\n",
    "metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
